{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Processes scraped data into time series observations of 6 weeks."
      ],
      "metadata": {
        "id": "G_U9nkpNstZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt"
      ],
      "metadata": {
        "id": "9ABcty6sY0Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_weeks = 6"
      ],
      "metadata": {
        "id": "KFVvYqBjssh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_column_names(num_weeks):\n",
        "    \"\"\"\n",
        "    Returns a list of column names for the processed data.\n",
        "    \"\"\"\n",
        "    column_names = []\n",
        "    for curr_day in range(1, num_weeks * 5 + 1):\n",
        "        column_names += [f\"day{curr_day}_open\", f\"day{curr_day}_high\", f\"day{curr_day}_low\", f\"day{curr_day}_close\", f\"day{curr_day}_adjclose\", f\"day{curr_day}_volume\"]\n",
        "    return column_names"
      ],
      "metadata": {
        "id": "Fh8ADJRSZDmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data(df, processed_df):\n",
        "    \"\"\"\n",
        "    Processes the data by generating time series data for 6 weeks at a time and adds it to the processed_df DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    for ticker in df['Ticker'].unique():\n",
        "        curr_df = df[df['Ticker'] == ticker].copy().reset_index()\n",
        "        curr_df.sort_values(by='Date', ascending=True, inplace=True)\n",
        "\n",
        "        while not curr_df.empty:\n",
        "            start_date = curr_df.iloc[0]['Date']\n",
        "\n",
        "            present = True\n",
        "            for i in range(num_weeks):\n",
        "                for j in range(5):\n",
        "                    if start_date + dt.timedelta(days=i * 7 + j) not in curr_df.values:\n",
        "                        present = False\n",
        "                        break\n",
        "                if not present:\n",
        "                    break\n",
        "\n",
        "            if present:\n",
        "                new_row = []\n",
        "\n",
        "                for i in range(num_weeks):\n",
        "                  for j in range(5):\n",
        "                      req_row = curr_df[curr_df['Date'] == start_date + dt.timedelta(days=i * 7 + j)].reset_index()\n",
        "                      new_row += req_row.loc[0, ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']].tolist()\n",
        "                      curr_df.drop(req_row.index, inplace=True)\n",
        "                      curr_df.reset_index(inplace=True)\n",
        "                      curr_df.drop(['index'], axis=1, inplace=True)\n",
        "\n",
        "                processed_df.loc[len(processed_df)] = [ticker, start_date] + new_row\n",
        "\n",
        "            else:\n",
        "                curr_df.drop(0, inplace=True)\n",
        "                curr_df.reset_index(inplace=True)\n",
        "                curr_df.drop(['index'], axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "6bBduWeRsD9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year = '2020'\n",
        "url = f'https://raw.githubusercontent.com/arushiag12/stock-price-prediction/main/data/scraped-data/S%26P500_data_{year}.csv'\n",
        "df = pd.read_csv(url).drop(['Unnamed: 0'], axis=1)\n",
        "df['Date'] = df['Date'].apply(lambda x: dt.datetime.strptime(x, '%Y-%m-%d'))\n",
        "\n",
        "processed_df = pd.DataFrame(columns = ['ticker', 'start_date'] + get_column_names(num_weeks))\n",
        "process_data(df, processed_df)\n",
        "processed_df.to_csv(f'data_{num_weeks}weeks_{year}.csv')"
      ],
      "metadata": {
        "id": "9Mh2yjnfY4Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year = '2021'\n",
        "url = f'https://raw.githubusercontent.com/arushiag12/stock-price-prediction/main/data/scraped-data/S%26P500_data_{year}.csv'\n",
        "df = pd.read_csv(url).drop(['Unnamed: 0'], axis=1)\n",
        "df['Date'] = df['Date'].apply(lambda x: dt.datetime.strptime(x, '%Y-%m-%d'))\n",
        "\n",
        "processed_df = pd.DataFrame(columns = ['ticker', 'start_date'] + get_column_names(num_weeks))\n",
        "process_data(df, processed_df)\n",
        "processed_df.to_csv(f'data_{num_weeks}weeks_{year}.csv')"
      ],
      "metadata": {
        "id": "yxtXrHsNtj5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year = '2022'\n",
        "url = f'https://raw.githubusercontent.com/arushiag12/stock-price-prediction/main/data/scraped-data/S%26P500_data_{year}.csv'\n",
        "df = pd.read_csv(url).drop(['Unnamed: 0'], axis=1)\n",
        "df['Date'] = df['Date'].apply(lambda x: dt.datetime.strptime(x, '%Y-%m-%d'))\n",
        "\n",
        "processed_df = pd.DataFrame(columns = ['ticker', 'start_date'] + get_column_names(num_weeks))\n",
        "process_data(df, processed_df)\n",
        "processed_df.to_csv(f'data_{num_weeks}weeks_{year}.csv')"
      ],
      "metadata": {
        "id": "Gt1cKe66tksu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year = '2023'\n",
        "url = f'https://raw.githubusercontent.com/arushiag12/stock-price-prediction/main/data/scraped-data/S%26P500_data_{year}.csv'\n",
        "df = pd.read_csv(url).drop(['Unnamed: 0'], axis=1)\n",
        "df['Date'] = df['Date'].apply(lambda x: dt.datetime.strptime(x, '%Y-%m-%d'))\n",
        "\n",
        "processed_df = pd.DataFrame(columns = ['ticker', 'start_date'] + get_column_names(num_weeks))\n",
        "process_data(df, processed_df)\n",
        "processed_df.to_csv(f'data_{num_weeks}weeks_{year}.csv')"
      ],
      "metadata": {
        "id": "2IvlnhQ_tlQ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}